{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Activation,BatchNormalization,Lambda,Input,Conv2D,Bidirectional,LSTM, Concatenate, MaxPooling2D,GlobalAveragePooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 30, 31, 29]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert_to_onehot(data):\n",
    "    #Creates a dict, that maps to every char of alphabet an unique int based on position\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    char_to_int = dict((c,i) for i,c in enumerate(alphabet))\n",
    "    \n",
    "    encoded_data = []\n",
    "    #Replaces every char in data with the mapped int\n",
    "    encoded_data.append([char_to_int[char] for char in data])\n",
    "    \n",
    "    \n",
    "\n",
    "    #This part now replaces the int by an one-hot array with size alphabet\n",
    "    one_hot = []\n",
    "    for value in encoded_data:\n",
    "        #At first, the whole array is initialized with 0\n",
    "        \n",
    "        for indexvalue in value:\n",
    "            letter = [0 for _ in range(len(alphabet))]\n",
    "            #Only at the number of the int, 1 is written\n",
    "\n",
    "            letter[indexvalue] = 1\n",
    "            \n",
    "            one_hot.append(letter)\n",
    "    return one_hot\n",
    "\n",
    "def convert_to_lable(data):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    return list(map(lambda x: alphabet.index(x), data))\n",
    "\n",
    "\n",
    "\n",
    "convert_to_lable('abcd453')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 4, 23, 19]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crnn():\n",
    "    x=image_input=Input(name='image_input',shape=[150,150,3])\n",
    "    \"\"\"\n",
    "    minx=Input(name='minx',shape=[1],dtype='float32')\n",
    "    miny=Input(name='miny',shape=[1],dtype='float32')\n",
    "    maxx=Input(name='maxx',shape=[1],dtype='float32')\n",
    "    maxy=Input(name='maxy',shape=[1],dtype='float32')\n",
    "    def cropimage(img,minx=0,miny=0,maxx=0,maxy=0):\n",
    "        \n",
    "        return tf.image.crop_to_bounding_box(img,minx,miny,maxx-minx,maxy-miny)\n",
    "    x=Lambda(cropimage)([x,minx,miny,maxx,maxy])\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    x=Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv1_1')(x)\n",
    "    x=MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1', padding='same')(x)\n",
    "    \n",
    "    x=Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv2_1')(x)\n",
    "    x=MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2', padding='same')(x)\n",
    "\n",
    "    x=Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3_1')(x)\n",
    "    x=Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3_2')(x)\n",
    "    x=MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3', padding='same')(x)\n",
    "    \n",
    "    x=Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4_1')(x)\n",
    "    x=BatchNormalization(name='batch1')(x)\n",
    "    \n",
    "    x=Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv5_1')(x)\n",
    "    x=BatchNormalization(name='batch2')(x)\n",
    "    x=MaxPooling2D(pool_size=(2, 2), strides=(1, 2), name='pool5', padding='valid')(x)\n",
    "    \n",
    "    x=Conv2D(512, (2, 2), strides=(1, 1), activation='relu', padding='valid', name='conv6_1')(x)\n",
    "    x=keras.layers.Reshape((-1,512))(x)\n",
    "    \n",
    "    x=Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x=Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    num_classes=36\n",
    "    x=keras.layers.Dense(num_classes, name='dense1')(x)#알파벳+숫자\n",
    "    \n",
    "    x=y_pred= Activation('softmax', name='softmax')(x)\n",
    "    \n",
    "   # model_pred=keras.models.Model(inputs=[image_input,minx,miny,maxx,maxy],outputs=x)\n",
    "    model_pred=keras.models.Model(inputs=image_input,outputs=x)\n",
    "    \n",
    "    maxstringlen=int(y_pred.shape[1])\n",
    "    \n",
    "    def ctc_lambda_func(args):\n",
    "        labels, y_pred, input_length, label_length = args\n",
    "        return K.ctc_batch_cost(labels, y_pred, input_length, label_length)    \n",
    "    \n",
    "    labels = Input(name='label_input', shape=[maxstringlen], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64') \n",
    "    \n",
    "    ctcloss=Lambda(ctc_lambda_func,output_shape=(1,),name='ctc')([labels,y_pred,input_length,label_length])\n",
    "    \n",
    "   # model_train=keras.models.Model(inputs=[image_input,minx,miny,maxx,maxy,labels,input_length,label_length],outputs=ctc_loss)\n",
    "    model_train=keras.models.Model(inputs=[image_input,labels,input_length,label_length],outputs=ctcloss)\n",
    "    \n",
    "    return model_train,model_pred,maxstringlen\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincrnn,predcrnn,inputlength=crnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropvalpath='D:/ocr/cropvalimage/'\n",
    "#croptrainpath='D:/ocr/croptrainimage/'\n",
    "valdf=pd.read_json('valcrnn.json',orient='index')\n",
    "valdf['label']=valdf['label'].apply(lambda x: convert_to_lable(x))\n",
    "traindf=pd.read_json('traincrnn.json',orient='index')\n",
    "traindf['label']=traindf['label'].apply(lambda x: convert_to_lable(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a7ad2bcb93d48576_2    [12, 14, 17, 6, 0, 13]\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdf.loc[valdf['cropimgid']=='a7ad2bcb93d48576_2']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cropimgid</th>\n",
       "      <th>label</th>\n",
       "      <th>imgid</th>\n",
       "      <th>labellangth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a7ad2bcb93d48576_1</th>\n",
       "      <td>a7ad2bcb93d48576_1</td>\n",
       "      <td>[17, 8, 2, 7, 0, 17, 3]</td>\n",
       "      <td>a7ad2bcb93d48576</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a7ad2bcb93d48576_2</th>\n",
       "      <td>a7ad2bcb93d48576_2</td>\n",
       "      <td>[12, 14, 17, 6, 0, 13]</td>\n",
       "      <td>a7ad2bcb93d48576</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a7ad2bcb93d48576_3</th>\n",
       "      <td>a7ad2bcb93d48576_3</td>\n",
       "      <td>[0, 11, 19, 4, 17, 4, 3]</td>\n",
       "      <td>a7ad2bcb93d48576</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a7ad2bcb93d48576_4</th>\n",
       "      <td>a7ad2bcb93d48576_4</td>\n",
       "      <td>[2, 0, 17, 1, 14, 13]</td>\n",
       "      <td>a7ad2bcb93d48576</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a7ad2bcb93d48576_5</th>\n",
       "      <td>a7ad2bcb93d48576_5</td>\n",
       "      <td>[0, 13]</td>\n",
       "      <td>a7ad2bcb93d48576</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607b023cd68febab_52</th>\n",
       "      <td>607b023cd68febab_52</td>\n",
       "      <td>[1, 14, 23, 4, 18]</td>\n",
       "      <td>607b023cd68febab</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607b023cd68febab_53</th>\n",
       "      <td>607b023cd68febab_53</td>\n",
       "      <td>[13, 4, 22]</td>\n",
       "      <td>607b023cd68febab</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607b023cd68febab_55</th>\n",
       "      <td>607b023cd68febab_55</td>\n",
       "      <td>[18, 19, 14, 13, 4, 18]</td>\n",
       "      <td>607b023cd68febab</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607b023cd68febab_61</th>\n",
       "      <td>607b023cd68febab_61</td>\n",
       "      <td>[12]</td>\n",
       "      <td>607b023cd68febab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607b023cd68febab_63</th>\n",
       "      <td>607b023cd68febab_63</td>\n",
       "      <td>[12]</td>\n",
       "      <td>607b023cd68febab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105729 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cropimgid                     label  \\\n",
       "a7ad2bcb93d48576_1    a7ad2bcb93d48576_1   [17, 8, 2, 7, 0, 17, 3]   \n",
       "a7ad2bcb93d48576_2    a7ad2bcb93d48576_2    [12, 14, 17, 6, 0, 13]   \n",
       "a7ad2bcb93d48576_3    a7ad2bcb93d48576_3  [0, 11, 19, 4, 17, 4, 3]   \n",
       "a7ad2bcb93d48576_4    a7ad2bcb93d48576_4     [2, 0, 17, 1, 14, 13]   \n",
       "a7ad2bcb93d48576_5    a7ad2bcb93d48576_5                   [0, 13]   \n",
       "...                                  ...                       ...   \n",
       "607b023cd68febab_52  607b023cd68febab_52        [1, 14, 23, 4, 18]   \n",
       "607b023cd68febab_53  607b023cd68febab_53               [13, 4, 22]   \n",
       "607b023cd68febab_55  607b023cd68febab_55   [18, 19, 14, 13, 4, 18]   \n",
       "607b023cd68febab_61  607b023cd68febab_61                      [12]   \n",
       "607b023cd68febab_63  607b023cd68febab_63                      [12]   \n",
       "\n",
       "                                imgid  labellangth  \n",
       "a7ad2bcb93d48576_1   a7ad2bcb93d48576            7  \n",
       "a7ad2bcb93d48576_2   a7ad2bcb93d48576            6  \n",
       "a7ad2bcb93d48576_3   a7ad2bcb93d48576            7  \n",
       "a7ad2bcb93d48576_4   a7ad2bcb93d48576            6  \n",
       "a7ad2bcb93d48576_5   a7ad2bcb93d48576            2  \n",
       "...                               ...          ...  \n",
       "607b023cd68febab_52  607b023cd68febab            5  \n",
       "607b023cd68febab_53  607b023cd68febab            3  \n",
       "607b023cd68febab_55  607b023cd68febab            6  \n",
       "607b023cd68febab_61  607b023cd68febab            1  \n",
       "607b023cd68febab_63  607b023cd68febab            1  \n",
       "\n",
       "[105729 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabel=traindf['label']\n",
    "vallabel=valdf['label']\n",
    "trainlabellength=traindf['labellangth']#a-e 스펠링틀림\n",
    "vallabellength=valdf['labellangth']\n",
    "trainlabel=trainlabel.reset_index()\n",
    "trainlabel=trainlabel['label']\n",
    "#vallabel= vallabel.sort_index()\n",
    "vallabel=vallabel.reset_index()\n",
    "#vallabel=vallabel['label']\n",
    "trainlabellength=trainlabellength.reset_index()\n",
    "trainlabellength=trainlabellength['labellangth']\n",
    "vallabellength=vallabellength.reset_index()\n",
    "vallabellength=vallabellength['labellangth']\n",
    "inputlengthlist=np.full(len(vallabel),inputlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vallabel['index']=vallabel['index'].apply(lambda x:cropvalpath+x+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105729 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "traindata=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    croptrainpath,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    image_size=(150,150),\n",
    "    smart_resize=True\n",
    ")\n",
    "\n",
    "valdata=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    cropvalpath,\n",
    "    #labels=list(vallabel),\n",
    "   # label_mode='categorical',\n",
    "    image_size=(150,150),\n",
    "    smart_resize=True,\n",
    "    #validation_split=0.2,\n",
    "    #subset='training',\n",
    "    seed=111,\n",
    "    batch_size=16\n",
    "    \n",
    ")\n",
    "\"\"\"\n",
    "valdatagen=tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2)\n",
    "valdata=valdatagen.flow_from_dataframe(vallabel,\n",
    "                               x_col='index',\n",
    "                               y_col='label',\n",
    "                               target_size=(150,150),\n",
    "                               color_mode='rgb',\n",
    "                               batch_size=16,\n",
    "                               interpolation='nearest',\n",
    "                               class_mode='raw'\n",
    "                               \n",
    "                              \n",
    "                              )\n",
    "#flowfromdataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([8, 5]) array([19,  9,  7, 13, 21]) array([4, 9, 5])\n",
      " array([ 9, 20]) array([20,  8,  1, 20]) array([13,  5, 20,  1, 12, 19])\n",
      " array([36, 36,  3]) array([22,  9,  3, 20, 15, 18,  9,  1]) array([30])\n",
      " array([ 1, 14, 20,  8, 15, 14, 25]) array([ 7, 18,  5,  5, 14])\n",
      " array([12, 15,  1,  4,  9, 14,  7]) array([15, 15, 19, 20])\n",
      " array([20,  8,  5]) array([19, 20, 15, 16]) array([ 1, 14,  4])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for images,i in valdata:\n",
    "  \n",
    "    print(i)\n",
    "    plt.imshow(images[0].astype(np.uint8))\n",
    "    break;\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([18,  9,  3,  8,  1, 18,  4]),\n",
       "       array([13, 15, 18,  7,  1, 14]),\n",
       "       array([ 1, 12, 20,  5, 18,  5,  4]), ...,\n",
       "       array([19, 20, 15, 14,  5, 19]), array([13]), array([13])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdata.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 11,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 11,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 12,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 11,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 11,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 10,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 13,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 10,\n",
       " 3,\n",
       " 11,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 12,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 10,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 14,\n",
       " 3,\n",
       " 4,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 17,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 11,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 19,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 13,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 17,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 20,\n",
       " 3,\n",
       " 15,\n",
       " 1,\n",
       " 9,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 11,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 11,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 12,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 13,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 13,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 10,\n",
       " 15,\n",
       " 12,\n",
       " 7,\n",
       " 4,\n",
       " 10,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 8,\n",
       " 6,\n",
       " 15,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 11,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 15,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 10,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 10,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 11,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 12,\n",
       " 2,\n",
       " 11,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 10,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 10,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 10,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vallabellength=[]\n",
    "for i in valdata.labels:\n",
    "    vallabellength.append(len(i))\n",
    "vallabellength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincrnn.compile(optimizer='adam',loss={'ctc':lambda labels,y_pred:y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard  ## TensorBoard 를 import합니다.\n",
    "modelver='crnn_train_v1'\n",
    "checkdir = './checkpoints/' + datetime.datetime.now().strftime('%Y%m%d%H%M') + '_' + modelver\n",
    "if not os.path.exists(checkdir):\n",
    "    os.makedirs(checkdir)\n",
    "\n",
    "with open(checkdir+'/source.py','wb') as f:\n",
    "    source = ''.join(['# In[%i]\\n%s\\n\\n' % (i, In[i]) for i in range(len(In))])\n",
    "    f.write(source.encode())\n",
    "log_dir = \"./logs/fit/\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard =TensorBoard(log_dir=log_dir,histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x, y, input_length,label_length,batchsize):\n",
    "    sample_idx = 0\n",
    "    while True:\n",
    "        for row in range(batchsize):\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'tensorflow.python.keras.preprocessing.image.DataFrameIterator'>\", \"<class 'numpy.ndarray'>\"}), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-ca5e16600de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m               callbacks=[keras.callbacks.ModelCheckpoint(checkdir+'/weights.{epoch:03d}.h5', verbose=1, save_weights_only=True),\n\u001b[1;32m----> 7\u001b[1;33m                          tensorboard]\n\u001b[0m\u001b[0;32m      8\u001b[0m              )\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1150\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     self._adapter = adapter_cls(\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    992\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 994\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    995\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'tensorflow.python.keras.preprocessing.image.DataFrameIterator'>\", \"<class 'numpy.ndarray'>\"}), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "#[image_input,labels,input_length,label_length]\n",
    "his=traincrnn.fit(x=[valdata,valdata.labels,np.array( inputlengthlist),np.array(vallabellength)],\n",
    "              y=valdata.labels,\n",
    "                batch_size=16, \n",
    "              epochs=15,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(checkdir+'/weights.{epoch:03d}.h5', verbose=1, save_weights_only=True),\n",
    "                         tensorboard]\n",
    "             )\n",
    "\n",
    "his=traincrnn.fit_generator(valdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'str\\\\\\'>\"})\\'})', \"<class 'int'>\", \"<class 'tensorflow.python.framework.ops.EagerTensor'>\"}), <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-131957d0d6e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtesttake\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdsa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraincrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtesttake\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvallabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvallabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#dsa=traincrnn.predict([testtake,[1.,2.,3.,],inputlength,3])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1704\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1706\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1150\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     self._adapter = adapter_cls(\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    992\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 994\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    995\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'str\\\\\\'>\"})\\'})', \"<class 'int'>\", \"<class 'tensorflow.python.framework.ops.EagerTensor'>\"}), <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "for i in valdata.take(1):\n",
    "    testtake=i\n",
    "    break\n",
    "dsa=traincrnn.predict([testtake,[vallabel[1]],inputlength,len(vallabel)])\n",
    "#dsa=traincrnn.predict([testtake,[1.,2.,3.,],inputlength,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 54.         53.         48.       ]\n",
      "   [ 54.         53.         48.       ]\n",
      "   [ 54.         53.         48.       ]\n",
      "   ...\n",
      "   [ 78.        100.         28.       ]\n",
      "   [ 78.        100.         28.       ]\n",
      "   [ 78.        100.         28.       ]]\n",
      "\n",
      "  [[ 54.         53.         48.       ]\n",
      "   [ 54.         53.         48.       ]\n",
      "   [ 54.         53.         48.       ]\n",
      "   ...\n",
      "   [ 78.        100.         28.       ]\n",
      "   [ 78.        100.         28.       ]\n",
      "   [ 78.        100.         28.       ]]\n",
      "\n",
      "  [[ 54.         53.         48.       ]\n",
      "   [ 54.         53.         48.       ]\n",
      "   [ 54.         53.         48.       ]\n",
      "   ...\n",
      "   [ 78.        100.         28.       ]\n",
      "   [ 78.        100.         28.       ]\n",
      "   [ 78.        100.         28.       ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 30.         30.         42.       ]\n",
      "   [ 30.         30.         42.       ]\n",
      "   [ 30.         30.         42.       ]\n",
      "   ...\n",
      "   [ 63.         62.         42.       ]\n",
      "   [ 63.         62.         42.       ]\n",
      "   [ 63.         62.         42.       ]]\n",
      "\n",
      "  [[ 30.         30.         42.       ]\n",
      "   [ 30.         30.         42.       ]\n",
      "   [ 30.         30.         42.       ]\n",
      "   ...\n",
      "   [ 63.         62.         42.       ]\n",
      "   [ 63.         62.         42.       ]\n",
      "   [ 63.         62.         42.       ]]\n",
      "\n",
      "  [[ 30.         30.         42.       ]\n",
      "   [ 30.         30.         42.       ]\n",
      "   [ 30.         30.         42.       ]\n",
      "   ...\n",
      "   [ 63.         62.         42.       ]\n",
      "   [ 63.         62.         42.       ]\n",
      "   [ 63.         62.         42.       ]]]\n",
      "\n",
      "\n",
      " [[[238.        238.        238.       ]\n",
      "   [238.5       238.5       238.5      ]\n",
      "   [239.        239.        239.       ]\n",
      "   ...\n",
      "   [238.16666   238.16666   238.16666  ]\n",
      "   [237.        237.        237.       ]\n",
      "   [236.        236.        236.       ]]\n",
      "\n",
      "  [[238.        238.        238.       ]\n",
      "   [238.5       238.5       238.5      ]\n",
      "   [239.        239.        239.       ]\n",
      "   ...\n",
      "   [238.16666   238.16666   238.16666  ]\n",
      "   [237.75      237.75      237.75     ]\n",
      "   [237.5       237.5       237.5      ]]\n",
      "\n",
      "  [[238.        238.        238.       ]\n",
      "   [238.41667   238.41667   238.41667  ]\n",
      "   [238.83333   238.83333   238.83333  ]\n",
      "   ...\n",
      "   [238.16666   238.16666   238.16666  ]\n",
      "   [238.58333   238.58333   238.58333  ]\n",
      "   [239.16667   239.16667   239.16667  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[243.        243.        243.       ]\n",
      "   [242.5       242.5       242.5      ]\n",
      "   [241.83333   241.83333   241.83333  ]\n",
      "   ...\n",
      "   [238.16666   238.16666   238.16666  ]\n",
      "   [239.        239.        239.       ]\n",
      "   [240.        240.        240.       ]]\n",
      "\n",
      "  [[242.5       242.5       242.5      ]\n",
      "   [242.25      242.25      242.25     ]\n",
      "   [241.91666   241.91666   241.91666  ]\n",
      "   ...\n",
      "   [238.16666   238.16666   238.16666  ]\n",
      "   [239.        239.        239.       ]\n",
      "   [240.        240.        240.       ]]\n",
      "\n",
      "  [[242.        242.        242.       ]\n",
      "   [242.        242.        242.       ]\n",
      "   [242.        242.        242.       ]\n",
      "   ...\n",
      "   [238.16666   238.16666   238.16666  ]\n",
      "   [239.        239.        239.       ]\n",
      "   [240.        240.        240.       ]]]\n",
      "\n",
      "\n",
      " [[[255.        253.        250.       ]\n",
      "   [255.        253.        250.       ]\n",
      "   [255.        253.        250.       ]\n",
      "   ...\n",
      "   [246.        246.        246.       ]\n",
      "   [246.        246.        246.       ]\n",
      "   [246.        246.        246.       ]]\n",
      "\n",
      "  [[255.        253.        250.       ]\n",
      "   [255.        253.        250.       ]\n",
      "   [255.        253.        250.       ]\n",
      "   ...\n",
      "   [246.        246.        246.       ]\n",
      "   [246.        246.        246.       ]\n",
      "   [246.        246.        246.       ]]\n",
      "\n",
      "  [[255.        253.        250.       ]\n",
      "   [255.        253.        250.       ]\n",
      "   [255.        253.        250.       ]\n",
      "   ...\n",
      "   [246.        246.        246.       ]\n",
      "   [246.        246.        246.       ]\n",
      "   [246.        246.        246.       ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[240.        251.        253.       ]\n",
      "   [240.        251.        253.       ]\n",
      "   [240.        251.        253.       ]\n",
      "   ...\n",
      "   [244.        255.        255.       ]\n",
      "   [244.        255.        255.       ]\n",
      "   [244.        255.        255.       ]]\n",
      "\n",
      "  [[240.        251.        253.       ]\n",
      "   [240.        251.        253.       ]\n",
      "   [240.        251.        253.       ]\n",
      "   ...\n",
      "   [244.        255.        255.       ]\n",
      "   [244.        255.        255.       ]\n",
      "   [244.        255.        255.       ]]\n",
      "\n",
      "  [[240.        251.        253.       ]\n",
      "   [240.        251.        253.       ]\n",
      "   [240.        251.        253.       ]\n",
      "   ...\n",
      "   [244.        255.        255.       ]\n",
      "   [244.        255.        255.       ]\n",
      "   [244.        255.        255.       ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 18.         22.         70.       ]\n",
      "   [ 18.         22.         70.       ]\n",
      "   [ 17.983334   21.983334   69.98333  ]\n",
      "   ...\n",
      "   [ 14.016666   25.033333   69.98334  ]\n",
      "   [ 14.         25.         70.       ]\n",
      "   [ 14.         25.         70.       ]]\n",
      "\n",
      "  [[ 18.         22.         70.       ]\n",
      "   [ 18.         22.         70.       ]\n",
      "   [ 17.983334   21.983334   69.98333  ]\n",
      "   ...\n",
      "   [ 14.016666   25.033333   69.98334  ]\n",
      "   [ 14.         25.         70.       ]\n",
      "   [ 14.         25.         70.       ]]\n",
      "\n",
      "  [[ 17.966667   21.966667   69.95     ]\n",
      "   [ 17.966667   21.966667   69.95     ]\n",
      "   [ 17.95       21.95       69.93333  ]\n",
      "   ...\n",
      "   [ 14.         25.033054   69.93389  ]\n",
      "   [ 13.983334   25.         69.95     ]\n",
      "   [ 13.983334   25.         69.95     ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  3.0499992  16.05       61.05     ]\n",
      "   [  3.0499992  16.05       61.05     ]\n",
      "   [  3.0499992  16.05       61.05     ]\n",
      "   ...\n",
      "   [ 13.033055   24.999722   74.983055 ]\n",
      "   [ 12.983334   24.983334   74.95     ]\n",
      "   [ 12.983334   24.983334   74.95     ]]\n",
      "\n",
      "  [[  3.         16.         61.       ]\n",
      "   [  3.         16.         61.       ]\n",
      "   [  3.         16.         61.       ]\n",
      "   ...\n",
      "   [ 13.049999   25.016666   75.03333  ]\n",
      "   [ 13.         25.         75.       ]\n",
      "   [ 13.         25.         75.       ]]\n",
      "\n",
      "  [[  3.         16.         61.       ]\n",
      "   [  3.         16.         61.       ]\n",
      "   [  3.         16.         61.       ]\n",
      "   ...\n",
      "   [ 13.049999   25.016666   75.03333  ]\n",
      "   [ 13.         25.         75.       ]\n",
      "   [ 13.         25.         75.       ]]]\n",
      "\n",
      "\n",
      " [[[106.         94.        134.       ]\n",
      "   [106.         94.        134.       ]\n",
      "   [106.         94.        134.       ]\n",
      "   ...\n",
      "   [255.        245.        253.       ]\n",
      "   [255.        245.        253.       ]\n",
      "   [255.        245.        253.       ]]\n",
      "\n",
      "  [[106.         94.        134.       ]\n",
      "   [106.         94.        134.       ]\n",
      "   [106.         94.        134.       ]\n",
      "   ...\n",
      "   [255.        245.        253.       ]\n",
      "   [255.        245.        253.       ]\n",
      "   [255.        245.        253.       ]]\n",
      "\n",
      "  [[106.         94.        134.       ]\n",
      "   [106.         94.        134.       ]\n",
      "   [106.         94.        134.       ]\n",
      "   ...\n",
      "   [255.        245.        253.       ]\n",
      "   [255.        245.        253.       ]\n",
      "   [255.        245.        253.       ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[148.        146.        195.       ]\n",
      "   [148.        146.        195.       ]\n",
      "   [148.        146.        195.       ]\n",
      "   ...\n",
      "   [185.        201.        224.       ]\n",
      "   [185.        201.        224.       ]\n",
      "   [185.        201.        224.       ]]\n",
      "\n",
      "  [[148.        146.        195.       ]\n",
      "   [148.        146.        195.       ]\n",
      "   [148.        146.        195.       ]\n",
      "   ...\n",
      "   [185.        201.        224.       ]\n",
      "   [185.        201.        224.       ]\n",
      "   [185.        201.        224.       ]]\n",
      "\n",
      "  [[148.        146.        195.       ]\n",
      "   [148.        146.        195.       ]\n",
      "   [148.        146.        195.       ]\n",
      "   ...\n",
      "   [185.        201.        224.       ]\n",
      "   [185.        201.        224.       ]\n",
      "   [185.        201.        224.       ]]]\n",
      "\n",
      "\n",
      " [[[ 37.         45.         68.       ]\n",
      "   [ 37.         45.         68.       ]\n",
      "   [ 37.         45.         68.       ]\n",
      "   ...\n",
      "   [130.        143.        177.       ]\n",
      "   [130.        143.        177.       ]\n",
      "   [130.        143.        177.       ]]\n",
      "\n",
      "  [[ 37.         45.         68.       ]\n",
      "   [ 37.         45.         68.       ]\n",
      "   [ 37.         45.         68.       ]\n",
      "   ...\n",
      "   [130.        143.        177.       ]\n",
      "   [130.        143.        177.       ]\n",
      "   [130.        143.        177.       ]]\n",
      "\n",
      "  [[ 37.         45.         68.       ]\n",
      "   [ 37.         45.         68.       ]\n",
      "   [ 37.         45.         68.       ]\n",
      "   ...\n",
      "   [130.        143.        177.       ]\n",
      "   [130.        143.        177.       ]\n",
      "   [130.        143.        177.       ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 47.         31.         68.       ]\n",
      "   [ 47.         31.         68.       ]\n",
      "   [ 47.         31.         68.       ]\n",
      "   ...\n",
      "   [ 28.         30.         53.       ]\n",
      "   [ 28.         30.         53.       ]\n",
      "   [ 28.         30.         53.       ]]\n",
      "\n",
      "  [[ 47.         31.         68.       ]\n",
      "   [ 47.         31.         68.       ]\n",
      "   [ 47.         31.         68.       ]\n",
      "   ...\n",
      "   [ 28.         30.         53.       ]\n",
      "   [ 28.         30.         53.       ]\n",
      "   [ 28.         30.         53.       ]]\n",
      "\n",
      "  [[ 47.         31.         68.       ]\n",
      "   [ 47.         31.         68.       ]\n",
      "   [ 47.         31.         68.       ]\n",
      "   ...\n",
      "   [ 28.         30.         53.       ]\n",
      "   [ 28.         30.         53.       ]\n",
      "   [ 28.         30.         53.       ]]]], shape=(32, 150, 150, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in valdata.take(1):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vallabel)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
